# -*- coding: utf-8 -*-
"""ImageClassification_LeNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t7SxwEW5z8DtIMZFU7Kuk-V2so5bDDr2
"""

import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 4
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

import matplotlib.pyplot as plt
import numpy as np

# functions to show an image


def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = next(dataiter)

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))

print(images.shape)

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        # print(x.shape)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()



import torch.nn as nn
import torch.nn.functional as F


class Net1(nn.Module):
    def __init__(self):
        super().__init__()
        self.acda1 = Conv_DCFD(3, 6, kernel_size=7, inter_kernel_size=7, padding=1, stride=1, bias=True)#.cuda()
        self.pool = nn.MaxPool2d(2, 2)
        self.acda2 = Conv_DCFD(6, 16, kernel_size=7, inter_kernel_size=7, padding=1, stride=1, bias=True)#.cuda()
        self.fc1 = nn.Linear(16 * 8 * 8, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        print('acda1 shape:',self.acda1(x).shape)
        x = self.pool(F.relu(self.acda1(x)))
        
        print('maxpool acda1 shape:',x.shape)    
        x = self.pool(F.relu(self.acda2(x)))
        print(x.shape)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        print(x.shape)
        
        # print(self.fc1(x).shape())
        y = self.fc1(x)
        print(y.shape)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net1 = Net1()

import torch.nn as nn
import torch.nn.functional as F


class Net2(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv0 = nn.Conv2d(3, 64, 3, stride = 1)
        self.maxpool = nn.MaxPool2d(3,2)

        self.conv11 = nn.Conv2d(64,64,3)
        self.conv12 = nn.Conv2d(64,64,3)

        self.conv21 = nn.Conv2d(64,128,3)
        self.conv22 = nn.Conv2d(128,128,3)
        self.conv23 = nn.Conv2d(128,64,3)
        
        self.conv31 = nn.Conv2d(128,128,1)
        self.acda1 = Conv_DCFD(128, 128, kernel_size=7, inter_kernel_size=7, padding=1, stride=1, bias=True)#.cuda()
        self.conv32 = nn.Conv2d(128,256,1)
        self.conv33 = nn.Conv2d(256,128,1)

        self.conv41 = nn.Conv2d(256,256,1)
        self.acda2 = Conv_DCFD(256, 256, kernel_size=7, inter_kernel_size=7, padding=1, stride=1, bias=True)
        self.conv42 = nn.Conv2d(256,512,1)
        self.conv43 = nn.Conv2d(512,256,1)

        self.avg = nn.AvgPool2d(3)

        self.fc1 = nn.Linear(512* 4 * 4, 1000)
        # self.fc2 = nn.Linear(120, 84)
        # self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = (F.relu(self.conv0(x)))
        #print(x.shape)
        x = F.relu(self.conv12(F.relu(self.conv11(x))))
        # print('11')
        x = F.relu(self.conv12(F.relu(self.conv11(x))))
        # print('12')
        # print(x.shape)
        x = F.relu(F.relu(self.conv22(F.relu(self.conv21(x)))))
        #print('21')
        x = F.relu(self.conv22(F.relu(self.conv22(x))))
        #print('22')
        x = F.relu(self.conv32(self.acda1(F.relu(self.conv31(x)))))
        x = self.conv32(self.acda1(self.conv33(x)))

        x = F.relu(self.conv42(self.acda2(F.relu(self.conv41(x)))))
        x = self.conv42(self.acda2(self.conv43(x)))

        x = self.avg(x)
        print('final shape',x.shape)
        
        # x = self.pool(F.relu(self.acda1(x)))
        # x = self.pool(F.relu(self.acda2(x)))
        # # print(x.shape)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        # x = F.relu(self.fc2(x))
        # x = self.fc3(x)
        return x


net2 = Net2()

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)

for epoch in range(5):  # loop over the dataset multiple times
    print(epoch)
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        print(i)
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data
        # print(inputs.shape)
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net2(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

print('Finished Training')

correct = 0
total = 0
# since we're not training, we don't need to calculate the gradients for our outputs
with torch.no_grad():
    for data in testloader:
        images, labels = data
        # calculate outputs by running images through the network
        outputs = net13(images)
        # the class with the highest energy is what we choose as prediction
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')

!pip install config

'''
The code is directly translated from the matlab code 
https://github.com/xycheng/DCFNet/blob/master/calculate_FB_bases.m
'''
import numpy as np 
from scipy import special
import pdb
from config import *

def cart2pol(x, y):
    rho = np.sqrt(x**2 + y**2)
    phi = np.arctan2(y, x)
    return (phi, rho)

def calculate_FB_bases(L1):
	maxK = (2 * L1 + 1)**2 - 1

	L = L1 + 1
	R = L1 + 0.5

	truncate_freq_factor = 1.5

	if L1 < 2:
		truncate_freq_factor = 2

	xx, yy = np.meshgrid(range(-L, L+1), range(-L, L+1))

	xx = xx/R
	yy = yy/R

	ugrid = np.concatenate([yy.reshape(-1,1), xx.reshape(-1,1)], 1)
	tgrid, rgrid = cart2pol(ugrid[:,0], ugrid[:,1])

	num_grid_points = ugrid.shape[0]

	kmax = 15

	bessel = np.load('./bessel.npy')

	B = bessel[(bessel[:,0] <=kmax) & (bessel[:,3]<= np.pi*R*truncate_freq_factor)]

	idxB = np.argsort(B[:,2])

	mu_ns = B[idxB, 2]**2

	ang_freqs = B[idxB, 0]
	rad_freqs = B[idxB, 1]
	R_ns = B[idxB, 2]

	num_kq_all = len(ang_freqs)
	max_ang_freqs = max(ang_freqs)

	Phi_ns=np.zeros((num_grid_points, num_kq_all), np.float32)

	Psi = []
	kq_Psi = []
	num_bases=0

	for i in range(B.shape[0]):
		ki = ang_freqs[i]
		qi = rad_freqs[i]
		rkqi = R_ns[i]

		r0grid=rgrid*R_ns[i]

		F = special.jv(ki, r0grid)

		Phi = 1./np.abs(special.jv(ki+1, R_ns[i]))*F

		Phi[rgrid >=1]=0

		Phi_ns[:, i] = Phi

		if ki == 0:
			Psi.append(Phi)
			kq_Psi.append([ki,qi,rkqi])
			num_bases = num_bases+1

		else:
			Psi.append(Phi*np.cos(ki*tgrid)*np.sqrt(2))
			Psi.append(Phi*np.sin(ki*tgrid)*np.sqrt(2))
			kq_Psi.append([ki,qi,rkqi])
			kq_Psi.append([ki,qi,rkqi])
			num_bases = num_bases+2

	Psi = np.array(Psi)
	kq_Psi = np.array(kq_Psi)

	num_bases = Psi.shape[1]

	if num_bases > maxK:
		Psi = Psi[:maxK]
		kq_Psi = kq_Psi[:maxK]
	num_bases = Psi.shape[0]
	p = Psi.reshape(num_bases, 2*L+1, 2*L+1).transpose(1,2,0)
	psi = p[1:-1, 1:-1, :]
	# print(psi.shape)
	psi = psi.reshape((2*L1+1)**2, num_bases)

	c = np.sqrt(np.sum(psi**2, 0).mean())

	psi = psi/c

	return psi, c, kq_Psi



!pip install fb



import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import matplotlib as plt
from torch.autograd import Variable

from torch.nn.parameter import Parameter
import math
import scipy as sp
import scipy.linalg as linalg
import numpy as np
import pdb
from fb import *
from torch.nn.utils import spectral_norm

class Bases_Drop(nn.Module):
    def __init__(self, p):
        super(Bases_Drop, self).__init__()
        self.p = p
    def forward(self, x):
        if self.training:
            assert len(x.shape) == 5
            N, M, L, H, W = x.shape
            mask = torch.ones((N, 1, L, H, W)).float().cuda()*(1-self.p)
            mask = torch.bernoulli(mask) * (1 / (1-self.p))
            x = x * mask
        return x

def bases_list(ks, num_bases):
    len_list = ks // 2
    # print(len_list)
    b_list = []
    for i in range(len_list):
        kernel_size = (i+1)*2+1
        # print(kernel_size)
        # kernel_size = kernel_size
        normed_bases, _, _ = calculate_FB_bases(i+1)
        # print(normed_bases.shape)
        normed_bases = normed_bases.transpose().reshape(-1, kernel_size, kernel_size).astype(np.float32)[:num_bases, ...]
        # print(normed_bases.shape)
        pad = len_list - (i+1)
        bases = torch.Tensor(normed_bases)
        bases = F.pad(bases, (pad, pad, pad, pad, 0, 0)).view(num_bases, ks*ks)
        # print(bases.shape)
        b_list.append(bases)
    a = torch.cat(b_list,0)
    # print(a.shape)
    # print('en')
    return torch.cat(b_list, 0)

class Conv_DCFD(nn.Module):
    __constants__ = ['kernel_size', 'stride', 'padding', 'num_bases',
                     'bases_grad', 'mode']
    def __init__(self, in_channels, out_channels, kernel_size, inter_kernel_size = 7, stride=1, padding=0,   ### = 7
        num_bases=6, bias=True,  bases_grad=True, dilation=1, groups=1,
        mode='mode1', bases_drop=None):
        super(Conv_DCFD, self).__init__()
        self.in_channels = in_channels
        self.inter_kernel_size = inter_kernel_size
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.num_bases = num_bases
        assert mode in ['mode0', 'mode1'], 'Only mode0 and mode1 are available at this moment.'
        self.mode = mode
        self.bases_grad = bases_grad
        self.dilation = dilation
        self.bases_drop = bases_drop
        self.groups = groups

        bases = bases_list(kernel_size, num_bases)
        self.register_buffer('bases', torch.Tensor(bases).float())
        self.tem_size = len(bases)
        # self.tem_size = bases.shape[1]
        # print(len(bases))
        bases_size = num_bases * len(bases)
        # bases_size = num_bases * bases.shape[1]
        # print(bases_size)
        inter = max(64, bases_size//2)
        self.bases_net = nn.Sequential(
            nn.Conv2d(in_channels, inter, kernel_size=self.kernel_size, padding=1, stride=stride), ######## changed the kernel_size from 3 to kernel_size
            nn.BatchNorm2d(inter),
            nn.Tanh(),
            nn.Conv2d(inter, bases_size, kernel_size=3, padding=1),  ####### keep the kernel_size fixed as 3
            nn.BatchNorm2d(bases_size),
            nn.Tanh()
            )

        self.coef = Parameter(torch.Tensor(out_channels, in_channels*num_bases, 1, 1))

        if bias:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
        self.drop = Bases_Drop(p=0.1)


    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.coef.size(1))

        nn.init.kaiming_normal_(self.coef, mode='fan_out', nonlinearity='relu')
        if self.bias is not None:
            self.bias.data.zero_()


    def forward(self, input):
        N, C, H, W = input.shape
        # H = H//self.stride
        # W = W//self.stride
        H = (H + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1)//self.stride + 1
        W = (W + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1)//self.stride + 1
        # print(H)
        # print(W)
        M = self.num_bases

        drop_rate = 0.0
        # print('behen')
        bbb = F.dropout2d(input, p=drop_rate, training=self.training)
        # print(bbb.shape)
        aaa = self.bases_net(F.dropout2d(input, p=drop_rate, training=self.training))
        # print(aaa.shape)
        # print('bhai')
        
        bases = self.bases_net(F.dropout2d(input, p=drop_rate, training=self.training)).view(N, self.num_bases, self.tem_size, H, W) # BxMxMxHxW
        #print(bases.shape)

        #print(2)
        self.bases_coef = bases.cpu().data.numpy()
        bases = torch.einsum('bmkhw, kl->bmlhw', bases, self.bases)
        self.bases_save = bases.cpu().data.numpy()
        #print(bases.shape)
        x = F.unfold(F.dropout2d(input, p=drop_rate, training=self.training), kernel_size=self.kernel_size, stride=self.stride, padding=self.padding).view(N, self.in_channels, self.kernel_size*self.kernel_size, H, W)
        #print(x.shape)
        bases_out = torch.einsum('bmlhw, bclhw-> bcmhw', bases.view(N, self.num_bases, -1, H, W), x).reshape(N, self.in_channels*self.num_bases, H, W)
        bases_out = F.dropout2d(bases_out, p=drop_rate, training=self.training)
        #print(bases_out.shape)
        # out = F.conv2d(bases_out, self.coef, self.bias,stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups)
        pad = ((self.kernel_size - 3)/2)//self.stride
        pad = int(pad)
        #print('padding:',pad)
        out = F.conv2d(bases_out, self.coef, self.bias,stride=self.stride, padding=pad, dilation=self.dilation, groups=self.groups)

        #print(out.shape)
        return out

    # def extra_repr(self):
    #     return 'kernel_size={kernel_size}, inter_kernel_size={inter_kernel_size}, stride={stride}, padding={padding}, num_bases={num_bases}' \
    #         ', bases_grad={bases_grad}, mode={mode}, bases_drop={bases_drop}, in_channels={in_channels}, out_channels={out_channels}'.format(**self.__dict__)


if __name__ == '__main__':
    layer = Conv_DCFD(3, 10, kernel_size=7, inter_kernel_size=7, padding=1, stride=2, bias=True).cuda()
    # data = torch.randn(10 , 3, 224, 224).cuda()
    data = torch.randn(1,3,100,100).cuda()
    a = layer(data)
    # data = torch.randn(1,1,224,224).cuda()
    print(layer(data).shape)

print(data.shape)

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.nn.init as init
# import matplotlib as plt
# from torch.autograd import Variable

# from torch.nn.parameter import Parameter
# import math
# import scipy as sp
# import scipy.linalg as linalg
# import numpy as np
# import pdb
# from fb import *
# from torch.nn.utils import spectral_norm

# class Bases_Drop(nn.Module):
#     def __init__(self, p):
#         super(Bases_Drop, self).__init__()
#         self.p = p
#     def forward(self, x):
#         if self.training:
#             assert len(x.shape) == 5
#             N, M, L, H, W = x.shape
#             mask = torch.ones((N, 1, L, H, W)).float().cuda()*(1-self.p)
#             mask = torch.bernoulli(mask) * (1 / (1-self.p))
#             x = x * mask
#         return x

# def bases_list(ks, num_bases):
#     len_list = ks // 2
#     # print(len_list)
#     b_list = []
#     for i in range(len_list):
#         kernel_size = (i+1)*2+1
#         # print(kernel_size)
#         # kernel_size = kernel_size
#         normed_bases, _, _ = calculate_FB_bases(i+1)
#         # print(normed_bases.shape)
#         normed_bases = normed_bases.transpose().reshape(-1, kernel_size, kernel_size).astype(np.float32)[:num_bases, ...]
#         # print(normed_bases.shape)
#         pad = len_list - (i+1)
#         bases = torch.Tensor(normed_bases)
#         bases = F.pad(bases, (pad, pad, pad, pad, 0, 0)).view(num_bases, ks*ks)
#         # print(bases.shape)
#         b_list.append(bases)
#     a = torch.cat(b_list,0)
#     # print(a.shape)
#     # print('en')
#     return torch.cat(b_list, 0)

# class Conv_DCFD(nn.Module):
#     __constants__ = ['kernel_size', 'stride', 'padding', 'num_bases',
#                      'bases_grad', 'mode']
#     def __init__(self, in_channels, out_channels, kernel_size, inter_kernel_size = 7, stride=1, padding=0,   ### = 7
#         num_bases=6, bias=True,  bases_grad=True, dilation=1, groups=1,
#         mode='mode1', bases_drop=None):
#         super(Conv_DCFD, self).__init__()
#         self.in_channels = in_channels
#         self.inter_kernel_size = inter_kernel_size
#         self.out_channels = out_channels
#         self.kernel_size = kernel_size
#         self.stride = stride
#         self.padding = padding
#         self.num_bases = num_bases
#         assert mode in ['mode0', 'mode1'], 'Only mode0 and mode1 are available at this moment.'
#         self.mode = mode
#         self.bases_grad = bases_grad
#         self.dilation = dilation
#         self.bases_drop = bases_drop
#         self.groups = groups

#         bases = bases_list(kernel_size, num_bases)
#         self.register_buffer('bases', torch.Tensor(bases).float())
#         self.tem_size = len(bases)
#         # self.tem_size = bases.shape[1]
#         # print(len(bases))
#         bases_size = num_bases * len(bases)
#         # bases_size = num_bases * bases.shape[1]
#         # print(bases_size)
#         inter = max(64, bases_size//2)
#         self.bases_net = nn.Sequential(
#             nn.Conv2d(in_channels, inter, kernel_size=3, padding=1, stride=stride), ########
#             nn.BatchNorm2d(inter),
#             nn.Tanh(),
#             nn.Conv2d(inter, bases_size, kernel_size=3, padding=1),  #######
#             nn.BatchNorm2d(bases_size),
#             nn.Tanh()
#             )

#         self.coef = Parameter(torch.Tensor(out_channels, in_channels*num_bases, 1, 1))

#         if bias:
#             self.bias = Parameter(torch.Tensor(out_channels))
#         else:
#             self.register_parameter('bias', None)
#         self.reset_parameters()
#         self.drop = Bases_Drop(p=0.1)


#     def reset_parameters(self):
#         stdv = 1. / math.sqrt(self.coef.size(1))

#         nn.init.kaiming_normal_(self.coef, mode='fan_out', nonlinearity='relu')
#         if self.bias is not None:
#             self.bias.data.zero_()


#     def forward(self, input):
#         N, C, H, W = input.shape
#         # H = H//self.stride
#         # W = W//self.stride
#         H = (H + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1)//self.stride + 1
#         W = (W + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1)//self.stride + 1

#         M = self.num_bases

#         drop_rate = 0.0
#         bases = self.bases_net(F.dropout2d(input, p=drop_rate, training=self.training)).view(N, self.num_bases, self.tem_size, H, W) # BxMxMxHxW
#         # print(bases.shape)
#         # print(2)
#         self.bases_coef = bases.cpu().data.numpy()
#         bases = torch.einsum('bmkhw, kl->bmlhw', bases, self.bases)
#         self.bases_save = bases.cpu().data.numpy()
#         # print(bases.shape)
#         x = F.unfold(F.dropout2d(input, p=drop_rate, training=self.training), kernel_size=self.kernel_size, stride=self.stride, padding=self.padding).view(N, self.in_channels, self.kernel_size*self.kernel_size, H, W)
#         # print(x.shape)
#         bases_out = torch.einsum('bmlhw, bclhw-> bcmhw', bases.view(N, self.num_bases, -1, H, W), x).reshape(N, self.in_channels*self.num_bases, H, W)
#         bases_out = F.dropout2d(bases_out, p=drop_rate, training=self.training)
#         # print(bases_out.shape)
#         out = F.conv2d(bases_out, self.coef, self.bias,stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups)
#         # print(out.shape)
#         return out

#     # def extra_repr(self):
#     #     return 'kernel_size={kernel_size}, inter_kernel_size={inter_kernel_size}, stride={stride}, padding={padding}, num_bases={num_bases}' \
#     #         ', bases_grad={bases_grad}, mode={mode}, bases_drop={bases_drop}, in_channels={in_channels}, out_channels={out_channels}'.format(**self.__dict__)


# if __name__ == '__main__':
#     layer = Conv_DCFD(3, 10, kernel_size=3, inter_kernel_size=7, padding=1, stride=1, bias=True)#.cuda()
#     # data = torch.randn(10 , 3, 224, 224).cuda()
#     data = torch.randn(1,3,224,224)#.cuda()
#     a = layer(data)
#     # data = torch.randn(1,1,224,224).cuda()
# #     print(layer(data).shape)

# # print(data.shape)